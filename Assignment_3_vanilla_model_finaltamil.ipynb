{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 3 vanilla model_finaltamil.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnandVP123/q1/blob/main/Assignment_3_vanilla_model_finaltamil.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ5BwqDIYSP8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a156866-5c95-4c81-ea5e-36c57f5b36f2"
      },
      "source": [
        "!pip install wandb\n",
        "!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
        "!tar -xf dakshina_dataset_v1.0.tar\n",
        "#tamil"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.16-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 5.0 MB/s \n",
            "\u001b[?25hCollecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 53.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.11-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 48.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=9b0de4486aa98e0ff0e815a5ca2bdd9f7a11fc06e2707f8a10df650342cffd53\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.11 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 wandb-0.12.16\n",
            "--2022-05-08 17:20:33--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.97.128, 142.251.107.128, 74.125.141.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.97.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G   249MB/s    in 11s     \n",
            "\n",
            "2022-05-08 17:20:44 (167 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wtNj_IqYrW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b9458c4-d65b-4b0c-d710-185550c39f9c"
      },
      "source": [
        "##########processingdakshin#source of the code is mentioned in the report\n",
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "##classDP\n",
        "class DP():\n",
        "    def __init__(self, DATAPATH, source_lang = 'en', target_lang = \"ta\"):\n",
        "        self.source_lang = source_lang\n",
        "        self.target_lang = target_lang\n",
        "        self.trainpath = os.path.join(DATAPATH, target_lang, \"lexicons\", target_lang+\".translit.sampled.train.tsv\")\n",
        "        self.valpath = os.path.join(DATAPATH, target_lang, \"lexicons\", target_lang+\".translit.sampled.dev.tsv\")\n",
        "        self.testpath = os.path.join(DATAPATH, target_lang, \"lexicons\", target_lang+\".translit.sampled.test.tsv\")\n",
        "        self.train = pd.read_csv(self.trainpath,sep=\"\\t\",names=[\"tgt\", \"src\", \"count\"],)\n",
        "        self.val = pd.read_csv(self.valpath,sep=\"\\t\",names=[\"tgt\", \"src\", \"count\"],)\n",
        "        self.test = pd.read_csv(self.testpath,sep=\"\\t\",names=[\"tgt\", \"src\", \"count\"],)\n",
        "        ##Traijning of Data & creating val/test data \n",
        "        self.traindata = self.preprocess(self.train[\"src\"].to_list(), self.train[\"tgt\"].to_list())\n",
        "        (self.trencoderinput,self.train_decoder_input,self.train_decoder_target,self.source_vocab,self.target_vocab,) = self.traindata\n",
        "        self.so_chartoint, self.source_int2char = self.source_vocab\n",
        "        self.tar_chartoint, self.target_int2char = self.target_vocab\n",
        "        self.valdata = self.encode(self.val[\"src\"].to_list(),self.val[\"tgt\"].to_list(),list(self.so_chartoint.keys()),list(self.tar_chartoint.keys()),so_chartoint=self.so_chartoint,tar_chartoint=self.tar_chartoint,)\n",
        "        self.valencoderinput, self.val_decoder_input, self.val_decoder_target = self.valdata\n",
        "        self.so_chartoint, self.source_int2char = self.source_vocab\n",
        "        self.tar_chartoint, self.target_int2char = self.target_vocab\n",
        "        self.testdata = self.encode(self.test[\"src\"].to_list(),self.test[\"tgt\"].to_list(),list(self.so_chartoint.keys()),list(self.tar_chartoint.keys()),so_chartoint=self.so_chartoint,tar_chartoint=self.tar_chartoint,)\n",
        "        self.testencoderinput, self.test_decoder_input, self.test_decoder_target = self.testdata\n",
        "        self.so_chartoint, self.source_int2char = self.source_vocab\n",
        "        self.tar_chartoint, self.target_int2char = self.target_vocab\n",
        "    def dictionary_lookup(self, vocab):\n",
        "        char2int = dict([(char, i) for i, char in enumerate(vocab)])\n",
        "        int2char = dict((i, char) for char, i in char2int.items())\n",
        "        return char2int, int2char\n",
        "    def encode(self, source, target, source_chars, target_chars, so_chartoint=None, tar_chartoint=None):\n",
        "        num_encoder_tokens = len(source_chars)\n",
        "        numdecodertok = len(target_chars)\n",
        "        sourcelengthmax = max([len(txt) for txt in source])\n",
        "        targetlength = max([len(txt) for txt in target])\n",
        "        source_vocab, target_vocab = None, None\n",
        "        if so_chartoint == None and tar_chartoint == None:\n",
        "            print(\"Generating the dictionary lookups for character to integer mapping and back\")\n",
        "            so_chartoint, source_int2char = self.dictionary_lookup(source_chars)\n",
        "            tar_chartoint, target_int2char = self.dictionary_lookup(target_chars)\n",
        "            source_vocab = (so_chartoint, source_int2char)\n",
        "            target_vocab = (tar_chartoint, target_int2char)\n",
        "        encoderindata = np.zeros((len(source), sourcelengthmax, num_encoder_tokens), dtype=\"float32\")\n",
        "        decoderindata = np.zeros((len(source), targetlength, numdecodertok), dtype=\"float32\")\n",
        "        decodertardata = np.zeros((len(source), targetlength, numdecodertok), dtype=\"float32\")\n",
        "        for i, (input_text, target_text) in enumerate(zip(source, target)):\n",
        "            for t, char in enumerate(input_text):\n",
        "                encoderindata[i, t, so_chartoint[char]] = 1.0\n",
        "            encoderindata[i, t + 1 :, so_chartoint[\" \"]] = 1.0\n",
        "            for t, char in enumerate(target_text):\n",
        "                decoderindata[i, t, tar_chartoint[char]] = 1.0\n",
        "                if t > 0:\n",
        "                    decodertardata[i, t - 1, tar_chartoint[char]] = 1.0\n",
        "            decoderindata[i, t + 1 :, tar_chartoint[\" \"]] = 1.0\n",
        "            decodertardata[i, t:, tar_chartoint[\" \"]] = 1.0\n",
        "        if source_vocab != None and target_vocab != None:return (encoderindata,decoderindata,decodertardata,source_vocab,target_vocab,)\n",
        "        else:return encoderindata, decoderindata, decodertardata\n",
        "    def preprocess(self, source , target):\n",
        "        source_chars = set()\n",
        "        target_chars = set()\n",
        "        source = [str(x) for x in source]\n",
        "        target = [str(x) for x in target]\n",
        "        source_words = []\n",
        "        target_words = []\n",
        "        for src, tgt in zip(source, target):\n",
        "            tgt = \"\\t\" + tgt + \"\\n\"\n",
        "            source_words.append(src)\n",
        "            target_words.append(tgt)\n",
        "            for char in src:\n",
        "                if char not in source_chars:\n",
        "                    source_chars.add(char)\n",
        "            for char in tgt:\n",
        "                if char not in target_chars:\n",
        "                    target_chars.add(char)\n",
        "        target_chars = sorted(list(target_chars))\n",
        "        source_chars = sorted(list(source_chars))\n",
        "        source_chars.append(\" \")\n",
        "        target_chars.append(\" \")\n",
        "        num_encoder_tokens = len(source_chars)\n",
        "        print(\"Source Vocab length:\", num_encoder_tokens)\n",
        "        numdecodertok = len(target_chars)\n",
        "        print(\"Target Vocab length:\", numdecodertok)\n",
        "        sourcelengthmax = max([len(txt) for txt in source_words])\n",
        "        print(\"Max iplength:\", sourcelengthmax)\n",
        "        targetlength = max([len(txt) for txt in target_words])\n",
        "        print(\"Max seqlength:\", targetlength)\n",
        "        print(\"Number of samples:\", len(source))\n",
        "        return self.encode(source_words, target_words, source_chars, target_chars)\n",
        "#####processesing db for tamil\n",
        "DATAPATH = \"./dakshina_dataset_v1.0\"\n",
        "dataBase = DP(DATAPATH) \n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Vocab length: 27\n",
            "Target Vocab length: 49\n",
            "Max iplength: 30\n",
            "Max seqlength: 28\n",
            "Number of samples: 68218\n",
            "Generating the dictionary lookups for character to integer mapping and back\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT83-HQ3Z5fe"
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import load_model, Sequential,  Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, Input, InputLayer, Flatten, Activation, LSTM, SimpleRNN, GRU, TimeDistributed\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import wandb\n",
        "class SequencetoSequence():\n",
        "    def __init__(self, modelConfigDict, srcChar2Int, tgtChar2Int, using_pretrained_model = False):\n",
        "        #self.native_vocabulary = modelConfigDict[\"native_vocabulary\"]\n",
        "        self.numEn = modelConfigDict[\"numEn\"]\n",
        "        self.CT = modelConfigDict[\"CT\"]\n",
        "        self.Dimlat = modelConfigDict[\"Dimlat\"]\n",
        "        self.Dout = modelConfigDict[\"Dout\"]\n",
        "        self.numDe = modelConfigDict[\"numDe\"]\n",
        "        self.hidlayer = modelConfigDict[\"hidlayer\"]\n",
        "        self.tgtChar2Int = tgtChar2Int\n",
        "        self.srcChar2Int = srcChar2Int\n",
        "    def build_configurable_model(self):       \n",
        "        if self.CT == \"RNN\":\n",
        "            eninput = Input(shape=(None, len(self.srcChar2Int)))\n",
        "            enout = eninput\n",
        "            for i in range(1, self.numEn + 1):\n",
        "                encoder = SimpleRNN(self.Dimlat,return_state=True,return_sequences=True,Dout=self.Dout,)\n",
        "                enout, state = encoder(eninput)\n",
        "            encoderstates = [state]\n",
        "            decoderinput = Input(shape=(None, len(self.tgtChar2Int)))\n",
        "            deout = decoderinput\n",
        "            for i in range(1, self.numDe + 1):\n",
        "                decoder = SimpleRNN(self.Dimlat,return_sequences=True,return_state=True,Dout=self.Dout,)\n",
        "                deout, _ = decoder(decoderinput, initial_state=encoderstates)\n",
        "            hidlayer = Dense(self.hidlayer, activation=\"relu\")\n",
        "            hiddenout = hidlayer(deout)\n",
        "            decoddens = Dense(len(self.tgtChar2Int), activation=\"softmax\")\n",
        "            deout = decoddens(hiddenout)\n",
        "            model = Model([eninput, decoderinput], deout)\n",
        "            return model\n",
        "        elif self.CT == \"GRU\":\n",
        "            eninput = Input(shape=(None, len(self.srcChar2Int)))\n",
        "            enout = eninput\n",
        "            for i in range(1, self.numEn + 1):\n",
        "                encoder = GRU(self.Dimlat,return_state=True,return_sequences=True,Dout=self.Dout,)\n",
        "                enout, state = encoder(eninput)\n",
        "            encoderstates = [state]\n",
        "            decoderinput = Input(shape=(None, len(self.tgtChar2Int)))\n",
        "            deout = decoderinput\n",
        "            for i in range(1, self.numDe + 1):\n",
        "                decoder = GRU(self.Dimlat,return_sequences=True,return_state=True,Dout=self.Dout,)\n",
        "                deout, _ = decoder(decoderinput, initial_state=encoderstates)\n",
        "            hidlayer = Dense(self.hidlayer, activation=\"relu\")\n",
        "            hiddenout = hidlayer(deout)\n",
        "            decoddens = Dense(len(self.tgtChar2Int), activation=\"softmax\")\n",
        "            deout = decoddens(hiddenout)\n",
        "            model = Model([eninput, decoderinput], deout)\n",
        "            return model       \n",
        "        elif self.CT == \"LSTM\":\n",
        "            # encoder\n",
        "            eninput = Input(shape=(None, len(self.srcChar2Int)))\n",
        "            enout = eninput\n",
        "            for i in range(1, self.numEn + 1):\n",
        "                encoder = LSTM(self.Dimlat,return_state=True,return_sequences=True,Dout=self.Dout,)\n",
        "                enout, state_h, state_c = encoder(enout)\n",
        "            encoderstates = [state_h, state_c]\n",
        "            decoderinput = Input(shape=(None, len(self.tgtChar2Int)))\n",
        "            deout = decoderinput\n",
        "            for i in range(1, self.numDe + 1):\n",
        "                decoder = LSTM(self.Dimlat,return_state=True,return_sequences=True,Dout=self.Dout,)\n",
        "                deout, _, _ = decoder(deout, initial_state=encoderstates)\n",
        "            hidlayer = Dense(self.hidlayer, activation=\"relu\")\n",
        "            hiddenout = hidlayer(deout)\n",
        "            decoddens = Dense(len(self.tgtChar2Int), activation=\"softmax\")\n",
        "            deout = decoddens(hiddenout)\n",
        "            model = Model([eninput, decoderinput], deout)\n",
        "            return model\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-B1RkORaHjO"
      },
      "source": [
        "from tensorflow.keras.layers import RNN, LSTM, GRU, Dense\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import Input, Model\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "import tensorflow as tf\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "try:tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "except:\n",
        "    pass\n",
        "def train():\n",
        "    config_defaults = {\"celltype\": \"RNN\",\"Dim\": 256,\"hidlayer\": 128,\"optimi\": \"rmsprop\",\"numEncoders\": 1,\"numDecoders\": 1,\"dropout\": 0.2,\"epochs\": 1,\"batchsize\": 64,}\n",
        "    wandb.init(config=config_defaults,  project=\"cs6910_a3\", entity=\"ee20d064oe21d019\")\n",
        "    config = wandb.config\n",
        "    wandb.run.name = (str(config.CT)+ dataBase.source_lang+ str(config.numEn)+ \"_\"\n",
        "        + dataBase.target_lang+ \"_\"+ str(config.numDe)+ \"_\"+ config.optimiser+ \"_\"\n",
        "        + str(config.epochs)+ \"_\"+ str(config.Dout) + \"_\"+ str(config.batchsize)+ \"_\"+ str(config.Dimlat))\n",
        "    model = modelInit.build_configurable_model()\n",
        "    ##\n",
        "    modelInit = SequencetoSequence(config,srcChar2Int=dataBase.source_char2int, tgtChar2Int=dataBase.target_char2int)\n",
        "    model.summary()\n",
        "    #model.compile(optimizer=config.optimiser,loss=\"categorical_crossentropy\",metrics=[\"accuracy\"],)    \n",
        "    ##\n",
        "    wandb.run.save()\n",
        "    model.save(os.path.join(\"./TrainedModels\", wandb.run.name))    \n",
        "    wandb.finish()\n",
        "    ##\n",
        "    earlystopping = EarlyStopping(monitor=\"val_accuracy\", min_delta=0.01, patience=5, verbose=2, mode=\"auto\")\n",
        "    model.fit([dataBase.train_encoder_input, dataBase.train_decoder_input],dataBase.train_decoder_target,batchsize=config.batchsize,\n",
        "        epochs=config.epochs,validation_data=([dataBase.val_encoder_input, dataBase.val_decoder_input], dataBase.val_decoder_target),callbacks=[earlystopping, WandbCallback()],)\n",
        "    return model\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbZXjvV9agMF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "790c0637-697c-4337-fb0a-884604189205"
      },
      "source": [
        "sweep_config = {\"name\": \"Bayesian Sweep without attention\",\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
        "    \"parameters\": {\"CT\": {\"values\": [\"RNN\", \"GRU\", \"LSTM\"]},\"Dimlat\": {\"values\": [256]},\"hidlayer\": {\"values\": [128, 64]},\"optimiser\": {\"values\": [\"rmsprop\", \"adam\"]},\"numEn\": {\"values\": [1, 2, 3]},        \n",
        "        \"numDe\": {\"values\": [1, 2, 3]},\"Dout\": {\"values\": [0.1, 0.2, 0.3]},\"epochs\": {\"values\": [5,10,15, 20]},\"batchsize\": {\"values\": [32, 64]},},}\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"cs6910_a3\", entity=\"ee20d064oe21d019\")\n",
        "wandb.agent(sweep_id, train, count = 200)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: ptwhqqdn\n",
            "Sweep URL: https://wandb.ai/ee20d064oe21d019/cs6910_a3/sweeps/ptwhqqdn\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1mv0y64x with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tCT: RNN\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tDimlat: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tDout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidlayer: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumDe: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumEn: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: rmsprop\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mavp\u001b[0m (\u001b[33mee20d064oe21d019\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIHCjgp62Cur"
      },
      "source": [
        "#afterlinking gdrive\n",
        "#!cp -rf ./TrainedModels /content/gdrive/MyDrive/CS6910/Assignment3/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}