{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnandVP123/q1/blob/main/Assignmentee20d064oe21d019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XUJVyZ7utkI8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duxiEurVNypu",
        "outputId": "18880ad3-bcde-42af-e475-740ac1b86100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Wandb in /usr/local/lib/python3.7/dist-packages (0.12.10)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from Wandb) (1.0.8)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from Wandb) (1.15.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from Wandb) (3.1.27)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from Wandb) (5.4.8)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from Wandb) (7.1.2)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from Wandb) (0.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from Wandb) (3.13)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from Wandb) (0.4.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from Wandb) (3.17.3)\n",
            "Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from Wandb) (2.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from Wandb) (2.23.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from Wandb) (2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from Wandb) (2.8.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from Wandb) (1.5.6)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->Wandb) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->Wandb) (3.10.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->Wandb) (5.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->Wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->Wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->Wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->Wandb) (2021.10.8)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->Wandb) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "pip install Wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nxw67OTlOKJ3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.datasets import mnist\n",
        "import wandb\n",
        "\n",
        "# load dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "class_type = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'] \n",
        "\n",
        "proj_name='CS6910_ass1'\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "Cb6VNgmDWJmL",
        "outputId": "1679613d-8ecb-4857-a8f8-3ac790cab2c6",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Images for each Class :\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moe21d019\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/oe21d019/CS6910_ass1/runs/gre0um6s\" target=\"_blank\">deep-cherry-73</a></strong> to <a href=\"https://wandb.ai/oe21d019/CS6910_ass1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "\n",
        "\n",
        "print(\"Sample Images for each Class :\")\n",
        "class_list=list()\n",
        "wandb.init(project=proj_name)\n",
        "for i in range(10):\n",
        "  plt.subplot(2,5,i+1)\n",
        "  for j in range(len(y_train)):\n",
        "    if y_train[j] == i :\n",
        "        wandb.log({\"img\": [wandb.Image(x_train[j], caption=class_type[y_train[j]])]})\n",
        "        class_list.append(class_type[y_train[j]])\n",
        "        break    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5nwPsmBRClF"
      },
      "outputs": [],
      "source": [
        "def activate(x,activation):                                                           # Hidden layer activation function                                  \n",
        "\n",
        "  if activation == \"sigmoid\":\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "  elif activation == \"tanh\": \n",
        "    return np.tanh(x) \n",
        "\n",
        "  elif activation == \"relu\": \n",
        "    return x * (x > 0) \n",
        "\n",
        "def softmax(x):                                                                       # Output activation function\n",
        "    return np.exp(x) / np.sum(np.exp(x)) \n",
        "\n",
        "def feed_forward(x,parameters,sizes,activation):                                # feed-forward data through the network to estimate output\n",
        "  \n",
        "  H = {}\n",
        "  A={}\n",
        "  H[0] = x\n",
        " \n",
        "  for i in range(1,len(sizes)-1):\n",
        "    W = parameters[\"W\"+str(i)]\n",
        "    b = parameters[\"b\"+str(i)]\n",
        "    A[i] = np.dot(W,H[i-1])+b\n",
        "    H[i] = activate(A[i],activation)\n",
        "    \n",
        "  W = parameters[\"W\"+str(len(sizes)-1)]\n",
        "  b = parameters[\"b\"+str(len(sizes)-1)]\n",
        "  A[len(sizes)-1] = np.dot(W,H[len(sizes)-2])+b\n",
        "  #print(\"A:\",A[2],\"H:\",H[3],len(sizes))\n",
        "\n",
        "  y_hat = softmax(A[len(sizes)-1])\n",
        "  #print(y_hat)\n",
        "  \n",
        "  return y_hat,A,H\n",
        "\n",
        "def loss_compute(y,y_hat,parameters,loss_type,reg,sizes):                                               # function to compute the loss/error (both squared error and cross entropy)\n",
        "\n",
        "  if (loss_type == \"squared_error\"):\n",
        "    error = np.sum((y-y_hat)**2)/(2*len(y))\n",
        "  elif (loss_type == \"cross_entropy\") :\n",
        "    error = -1*np.sum(np.multiply(y,np.log(y_hat)))/len(y)\n",
        "\t\t\n",
        "  reg_error = 0.0                                                                        # account for regularization to avoid overfit of data - L2 norm regularization\n",
        "  for i in range(1,len(sizes)) :\n",
        "    reg_error = reg_error + (reg/2)*(np.sum(np.square(parameters[\"W\"+str(i)]))) \n",
        "  error = error + reg_error\n",
        "\n",
        "  return error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mip931SPRGnA"
      },
      "outputs": [],
      "source": [
        "def derivative(x,activation):                                                            # function to compute the derivative of hidden layer activation fn\n",
        "\n",
        "  if activation == \"sigmoid\":\n",
        "    return activate(x,\"sigmoid\")*(1-activate(x,\"sigmoid\"))\n",
        "  elif activation == \"tanh\": \n",
        "    return 1. - x * x \n",
        "  elif activation == \"relu\": \n",
        "    return 1. * (x > 0)\n",
        "\n",
        "def grad_init(sizes):\n",
        "  \n",
        "  grads={}\n",
        "  layers=len(sizes)\n",
        "  for i in range(1,layers):\n",
        "    grads[\"dW\" + str(i)] = np.zeros((sizes[i], sizes[i-1]))\n",
        "    grads[\"db\" + str(i)] = np.zeros((sizes[i],1))\n",
        "\n",
        "  return grads  \n",
        "\n",
        "\n",
        "def back_prop(X,Y,Y_hat,prev_grad,A,H,parameters,sizes,loss_type,activation,reg) :               # back-propogation rule to compute the gradients of activation, pre-activation and parameters\n",
        "  \n",
        "  new_grad = {}\n",
        "  grads = {}\n",
        "  # grads = {\"dH0\":np.zeros((input_size,1)),\"dA0\":np.zeros((input_size,1))}\n",
        "  for i in range(1,len(sizes)):\n",
        "    grads[\"dW\" + str(i)] = np.zeros((sizes[i], sizes[i-1]))\n",
        "    grads[\"db\" + str(i)] = np.zeros((sizes[i],1))\n",
        "    grads[\"dA\" + str(i)] = np.zeros((sizes[i],1))\n",
        "    grads[\"dH\" + str(i)] = np.zeros((sizes[i],1))\n",
        "\n",
        "  if loss_type == \"squared_error\":\n",
        "    grads[\"dH\"+str(len(sizes)-1)] = (Y_hat-Y) \n",
        "    grads[\"dA\"+str(len(sizes)-1)] = (Y_hat - Y)*Y_hat - Y_hat*(np.dot(np.transpose((Y_hat-Y)), Y_hat))\n",
        "\n",
        "  elif loss_type==\"cross_entropy\" :\n",
        "    grads[\"dH\"+str(len(sizes)-1)] = -(Y/Y_hat) \n",
        "    grads[\"dA\"+str(len(sizes)-1)] = -(Y-Y_hat)\n",
        "\n",
        "  for i in range(len(sizes)-1, 0, -1):\n",
        "    grads[\"dW\" + str(i)] = np.dot(grads[\"dA\" + str(i)], np.transpose(H[i-1]))\n",
        "    grads[\"db\" + str(i)] = grads[\"dA\" + str(i)] \n",
        "    if i>1 :\n",
        "      grads[\"dH\" + str(i-1)] = np.dot(np.transpose(parameters[\"W\" + str(i)]), grads[\"dA\" + str(i)])\n",
        "      grads[\"dA\" + str(i-1)] = np.multiply((grads[\"dH\" + str(i-1)]),derivative(A[i-1],activation))\n",
        "    \n",
        "  for i in range(1,len(sizes)):\n",
        "    new_grad[\"dW\" + str(i)] = grads[\"dW\" + str(i)] + prev_grad[\"dW\" + str(i)]\n",
        "    new_grad[\"db\" + str(i)] = grads[\"db\" + str(i)] + prev_grad[\"db\" + str(i)]\n",
        "    \n",
        "  return new_grad  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fObXr05kRR42"
      },
      "outputs": [],
      "source": [
        "# Initializations\n",
        "\n",
        "# Data\n",
        "X_train = np.array(x_train.reshape(x_train.shape[0], 784,1))         # reshape 2-D data to 1-D\n",
        "X_test = np.array(x_test.reshape(x_test.shape[0], 784,1))            # reshape 2-D data to 1-D\n",
        "\n",
        "def normalize_data(x):                                               # normalize input data\n",
        "  x_norm = x.astype('float32')\n",
        "  x_norm = x_norm / 255.0  \n",
        "  return x_norm \n",
        "\n",
        "X_train = normalize_data(X_train)\n",
        "X_val = X_train[-6000:]                                             # validation set input\n",
        "X_train = X_train[0:54000]                                          # training set input\n",
        "X_test = normalize_data(X_test)                                     # test set input\n",
        "\n",
        "\n",
        "Y_train = np.zeros([len(y_train),10,1])\n",
        "Y_test = np.zeros([len(y_test),10,1])\n",
        "\n",
        "for i in range(len(y_train)):                                        # convert y from just a class number to an indicator vector (10x1)\n",
        "  y = np.zeros([10, 1])\n",
        "  y[y_train[i]] = 1.0\n",
        "  Y_train[i] = y\n",
        "\n",
        "Y_val = Y_train[-6000:]                                              # validation set output\n",
        "Y_train = Y_train[0:54000]                                           # training set output\n",
        "\n",
        "for i in range(len(y_test)):                                         # convert y from just a class number to an indicator vector (10x1)\n",
        "  y = np.zeros([10, 1])\n",
        "  y[y_test[i]] = 1.0\n",
        "  Y_test[i] = y                                                      # test set output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TgmxRdVRd_E"
      },
      "outputs": [],
      "source": [
        "def network_init(sizes,w_init):                            # function to initialize weights and biases\n",
        "  parameters = {}\n",
        "  for i in range(1, len(sizes)):\n",
        "    if w_init == \"xavier\" :\n",
        "      parameters[\"W\" + str(i)] = np.random.randn(sizes[i], sizes[i-1])*np.sqrt(2./(sizes[i] + sizes[i-1]))\n",
        "      parameters[\"b\" + str(i)] = np.zeros((sizes[i],1))\n",
        "    elif w_init == \"random\" :\n",
        "      parameters[\"W\" + str(i)] = 0.01*np.random.randn(sizes[i], sizes[i-1])\n",
        "      parameters[\"b\" + str(i)] = 0.01*np.random.randn(sizes[i],1)\n",
        "\n",
        "  return parameters  \n",
        "\n",
        "def update_init(sizes) :                                  # function to initialize update dictionary that changes the weights and biases\n",
        "  update = {}\n",
        "  for i in range(1,len(sizes)):\n",
        "   update[\"W\"+str(i)] = np.zeros((sizes[i],sizes[i-1]))\n",
        "   update[\"b\"+str(i)] = np.zeros((sizes[i],1))\n",
        "\n",
        "  return update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSAwrFdJRikN"
      },
      "outputs": [],
      "source": [
        "def val_loss(y,y_hat,loss_type):                                # function to compute the loss/error (both squared error and cross entropy)\n",
        "  l = 0\n",
        "  if (loss_type == \"squared_error\"):\n",
        "    l = np.sum((y-y_hat)**2)/(2*len(y))\n",
        "  elif (loss_type == \"cross_entropy\") :\n",
        "    l = -1*np.sum(np.multiply(y,np.log(y_hat)))/len(y)\n",
        "  return l \n",
        "\n",
        "def calcAccLoss(parameters,xArr,yArr,sizes,loss_type,activation,type=\"val\",regu=None):          #function to calculate accuracy and total loss of a model\n",
        "  acc=0.0\n",
        "  lossVal=0.0\n",
        "  for x,y in zip(xArr,yArr):\n",
        "    y_hat= feed_forward(x,parameters,sizes,activation)[0]\n",
        "    if y_hat.argmax()==y.argmax():\n",
        "      acc+=1\n",
        "    if type==\"val\":\n",
        "      lossVal+=val_loss(y,y_hat,loss_type)\n",
        "    elif type==\"trng\":\n",
        "      lossVal+=loss_compute(y,y_hat,parameters,loss_type,regu,sizes)\n",
        "  acc=acc/len(xArr)\n",
        "  return (acc,lossVal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgTzpOl4RpNL"
      },
      "outputs": [],
      "source": [
        "#momentum Gradient descent\n",
        "def momentum_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=False) :\n",
        "\n",
        "   steps=0                                                                                      #used to count number of updates to parameters\n",
        "   parameters = network_init(sizes,w_init)\n",
        "   update = update_init(sizes)\n",
        "   gamma = 0.9\n",
        "   for n in range(n_epoch):\n",
        "\n",
        "     for j in range(0, X_train.shape[0], minibatch_size):                                       #minibatch division \n",
        "       X_mini = X_train[j:j + minibatch_size]\n",
        "       Y_mini = Y_train[j:j + minibatch_size]\n",
        "       grads = grad_init(sizes)\n",
        "\n",
        "       for x,y in zip(X_mini,Y_mini):\n",
        "         y_hat,A,H = feed_forward(x,parameters,sizes,activation)                                 \n",
        "         grads = back_prop(x,y,y_hat,grads,A,H,parameters,sizes,loss_type,activation,reg)\n",
        "\n",
        "       for i in range(1,len(sizes)) :                                                           #updating the parameters\n",
        "         update[\"W\"+str(i)] = gamma*update[\"W\"+str(i)] + lr*grads[\"dW\"+str(i)]                  \n",
        "         update[\"b\"+str(i)] = gamma*update[\"b\"+str(i)] + lr*grads[\"db\"+str(i)]\n",
        "         parameters[\"W\"+str(i)] = (1-lr*reg)*parameters[\"W\"+str(i)] - update[\"W\"+str(i)]\n",
        "         parameters[\"b\"+str(i)] = (1-lr*reg)*parameters[\"b\"+str(i)] - update[\"b\"+str(i)]\n",
        "       \n",
        "       steps=steps+1\n",
        "       if steps==10000:                                                                         #log every 10000 updates\n",
        "        acc,lossTot=calcAccLoss(parameters,X_train,Y_train,sizes,loss_type,activation,type=\"trng\",regu=reg)          #calculating accuracy and loss\n",
        "        accVal,lossTotVal=calcAccLoss(parameters,X_val,Y_val,sizes,loss_type,activation)\n",
        "\n",
        "        wandb.log({\"Accuracy\":acc,\"Loss\":lossTot,\"Accuracy_val\":accVal,\"Loss_val\":lossTotVal,\"Epoch\":n,\"n_datatrain\":j + minibatch_size+n*54000}) #logging\n",
        "        steps=0\n",
        "  \n",
        "   return parameters        \n",
        "\n",
        "#Nesterov acceleracted GD\n",
        "def nesterov_accelerated_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=False) :\n",
        "  steps=0\n",
        "  parameters = network_init(sizes,w_init)\n",
        "  update = update_init(sizes)\n",
        "  gamma = 0.9\n",
        "  for n in range(n_epoch):\n",
        "\n",
        "    for j in range(0, X_train.shape[0], minibatch_size):\n",
        "      X_mini = X_train[j:j + minibatch_size]\n",
        "      Y_mini = Y_train[j:j + minibatch_size]\n",
        "      \n",
        "\n",
        "      grads = grad_init(sizes)\n",
        "      for i in range(1,len(sizes)):                                                       #perform update before back propogation\n",
        "        update[\"W\"+str(i)] = gamma*update[\"W\"+str(i)]\n",
        "        update[\"b\"+str(i)] = gamma*update[\"b\"+str(i)]\n",
        "        parameters[\"W\"+str(i)] = (1-lr*reg)*parameters[\"W\"+str(i)] - update[\"W\"+str(i)]\n",
        "        parameters[\"b\"+str(i)] = (1-lr*reg)*parameters[\"b\"+str(i)] - update[\"b\"+str(i)]        \n",
        "\n",
        "      for x,y in zip(X_mini,Y_mini):\n",
        "        y_hat,A,H = feed_forward(x,parameters,sizes,activation)\n",
        "        grads = back_prop(x,y,y_hat,grads,A,H,parameters,sizes,loss_type,activation,reg)\n",
        "        \n",
        "      for k in range(1,len(sizes)) :\n",
        "        update[\"W\"+str(k)] = gamma*update[\"W\"+str(k)] + lr*grads[\"dW\"+str(k)]\n",
        "        update[\"b\"+str(k)] = gamma*update[\"b\"+str(k)] + lr*grads[\"db\"+str(k)]\n",
        "        parameters[\"W\"+str(k)] = (1-lr*reg)*parameters[\"W\"+str(k)] - update[\"W\"+str(k)]\n",
        "        parameters[\"b\"+str(k)] = (1-lr*reg)*parameters[\"b\"+str(k)] - update[\"b\"+str(k)]\n",
        "      steps=steps+1\n",
        "      if steps==10000:\n",
        "        if log:\n",
        "          acc,lossTot=calcAccLoss(parameters,X_train,Y_train,sizes,loss_type,activation,type=\"trng\",regu=reg)     \n",
        "          accVal,lossTotVal=calcAccLoss(parameters,X_val,Y_val,sizes,loss_type,activation)\n",
        "\n",
        "          wandb.log({\"Accuracy\":acc,\"Loss\":lossTot,\"Accuracy_val\":accVal,\"Loss_val\":lossTotVal,\"Epoch\":n,\"n_datatrain\":j + minibatch_size+n*54000})\n",
        "        steps=0   \n",
        "    \n",
        "  return parameters\n",
        "\n",
        "#stochastic GD\n",
        "def stochastic_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=False) :\n",
        "\n",
        "  steps=0\n",
        "  parameters = network_init(sizes,w_init)\n",
        "  update = update_init(sizes)\n",
        "  gamma = 0.9\n",
        "  for n in range(n_epoch):\n",
        "\n",
        "    for j in range(0, X_train.shape[0], minibatch_size):                                        #minibatch division\n",
        "      X_mini = X_train[j:j + minibatch_size]\n",
        "      Y_mini = Y_train[j:j + minibatch_size]\n",
        "      \n",
        "      grads = grad_init(sizes)\n",
        "      for x,y in zip(X_mini,Y_mini):\n",
        "        y_hat,A,H = feed_forward(x,parameters,sizes,activation)\n",
        "        grads = back_prop(x,y,y_hat,grads,A,H,parameters,sizes,loss_type,activation,reg)\n",
        "        \n",
        "      for i in range(1,len(sizes)-1) :                                                          #updating the parameters\n",
        "        parameters[\"W\"+str(i)] = (1-lr*reg)*parameters[\"W\"+str(i)] - lr*grads[\"dW\"+str(i)]\n",
        "        parameters[\"b\"+str(i)] = (1-lr*reg)*parameters[\"b\"+str(i)] - lr*grads[\"db\"+str(i)]\n",
        "      steps=steps+1\n",
        "      if steps==10000:\n",
        "        if log:\n",
        "          acc,lossTot=calcAccLoss(parameters,X_train,Y_train,sizes,loss_type,activation,type=\"trng\",regu=reg)     \n",
        "          accVal,lossTotVal=calcAccLoss(parameters,X_val,Y_val,sizes,loss_type,activation)\n",
        "\n",
        "          wandb.log({\"Accuracy\":acc,\"Loss\":lossTot,\"Accuracy_val\":accVal,\"Loss_val\":lossTotVal,\"Epoch\":n,\"n_datatrain\":j + minibatch_size+n*54000})\n",
        "        steps=0   \n",
        "\n",
        "  return parameters        \n",
        "\n",
        "#rmsprop GD\n",
        "def rmsprop_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=False) :\n",
        "  \n",
        "  steps=0\n",
        "  parameters = network_init(sizes,w_init)\n",
        "  update = update_init(sizes)\n",
        "  v = update_init(sizes)\n",
        "\n",
        "  betal = 0.99 #check this\n",
        "  eps = 1e-8\n",
        "\n",
        "  for n in range(n_epoch):\n",
        "\n",
        "    for j in range(0, X_train.shape[0], minibatch_size):\n",
        "      X_mini = X_train[j:j + minibatch_size]\n",
        "      Y_mini = Y_train[j:j + minibatch_size]\n",
        "      \n",
        "      grads = grad_init(sizes)\n",
        "      for x,y in zip(X_mini,Y_mini):\n",
        "        y_hat,A,H = feed_forward(x,parameters,sizes,activation)\n",
        "        grads = back_prop(x,y,y_hat,grads,A,H,parameters,sizes,loss_type,activation,reg)\n",
        "\n",
        "      for i in range(1,len(sizes)-1) :                                                                   #updating the parameters\n",
        "        v[\"W\"+str(i)] = betal*v[\"W\"+str(i)] + (1-betal)*grads[\"dW\"+str(i)]**2                            #v_w update  \n",
        "        v[\"b\"+str(i)] = betal*v[\"b\"+str(i)] + (1-betal)*grads[\"db\"+str(i)]**2                            #v_b update\n",
        "\n",
        "        update[\"W\"+str(i)]=lr*np.multiply(np.reciprocal(np.sqrt(v[\"W\"+str(i)]+eps)),grads[\"dW\"+str(i)])                 \n",
        "        update[\"b\"+str(i)]=lr*np.multiply(np.reciprocal(np.sqrt(v[\"b\"+str(i)]+eps)),grads[\"db\"+str(i)])\n",
        "        \n",
        "        parameters[\"W\"+str(i)] = (1-lr*reg)*parameters[\"W\"+str(i)] - update[\"W\"+str(i)]\n",
        "        parameters[\"b\"+str(i)] = (1-lr*reg)*parameters[\"b\"+str(i)] - update[\"b\"+str(i)]\n",
        "      steps=steps+1\n",
        "      if steps==10000:\n",
        "        if log:\n",
        "          acc,lossTot=calcAccLoss(parameters,X_train,Y_train,sizes,loss_type,activation,type=\"trng\",regu=reg)     \n",
        "          accVal,lossTotVal=calcAccLoss(parameters,X_val,Y_val,sizes,loss_type,activation)\n",
        "\n",
        "          wandb.log({\"Accuracy\":acc,\"Loss\":lossTot,\"Accuracy_val\":accVal,\"Loss_val\":lossTotVal,\"Epoch\":n,\"n_datatrain\":j + minibatch_size+n*54000})\n",
        "        steps=0   \n",
        "\n",
        "  return parameters          \n",
        "\n",
        "#Adam GD\n",
        "def adam_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=False) :\n",
        "  steps=0\n",
        "  parameters = network_init(sizes,w_init)\n",
        "  update = update_init(sizes)\n",
        "  m = update_init(sizes)\n",
        "  v = update_init(sizes)\n",
        "\n",
        "  beta1 = 0.9\n",
        "  beta2 = 0.999\n",
        "  eps = 1e-8\n",
        "  for n in range(n_epoch):\n",
        "\n",
        "    for j in range(0, X_train.shape[0], minibatch_size):\n",
        "      X_mini = X_train[j:j + minibatch_size]\n",
        "      Y_mini = Y_train[j:j + minibatch_size]\n",
        "      \n",
        "      grads = grad_init(sizes)                                          \n",
        "      for x,y in zip(X_mini,Y_mini):\n",
        "        y_hat,A,H = feed_forward(x,parameters,sizes,activation)\n",
        "        grads = back_prop(x,y,y_hat,grads,A,H,parameters,sizes,loss_type,activation,reg)\n",
        "        \n",
        "      for i in range(1,len(sizes)-1) :                                                    #updating the parameters\n",
        "        m[\"W\"+str(i)] = beta1*m[\"W\"+str(i)] + (1-beta1)*grads[\"dW\"+str(i)]                #m_w update\n",
        "        m[\"b\"+str(i)] = beta1*m[\"b\"+str(i)] + (1-beta1)*grads[\"db\"+str(i)]                #m_b update\n",
        "\n",
        "        v[\"W\"+str(i)] = beta2*v[\"W\"+str(i)] + (1-beta2)*grads[\"dW\"+str(i)]**2             #v_w update    \n",
        "        v[\"b\"+str(i)] = beta2*v[\"b\"+str(i)] + (1-beta2)*grads[\"db\"+str(i)]**2             #v_b update\n",
        "\n",
        "        #cumulative average\n",
        "        m_w_hat = m[\"W\"+str(i)]/(1-np.power(beta1,n+1))                                   \n",
        "        m_b_hat = m[\"b\"+str(i)]/(1-np.power(beta1,n+1))\n",
        "        v_w_hat = v[\"W\"+str(i)]/(1-np.power(beta2,n+1))\n",
        "        v_b_hat = v[\"b\"+str(i)]/(1-np.power(beta2,n+1))\n",
        "\n",
        "\n",
        "        update[\"W\"+str(i)]=lr*np.multiply(np.reciprocal(np.sqrt(v_w_hat+eps)),m_w_hat)\n",
        "        update[\"b\"+str(i)]=lr*np.multiply(np.reciprocal(np.sqrt(v_b_hat+eps)),m_b_hat)\n",
        "\n",
        "        parameters[\"W\"+str(i)] = (1-lr*reg)*parameters[\"W\"+str(i)] - update[\"W\"+str(i)]\n",
        "        parameters[\"b\"+str(i)] = (1-lr*reg)*parameters[\"b\"+str(i)] - update[\"b\"+str(i)]\n",
        "      \n",
        "    \n",
        "      steps=steps+1\n",
        "      if steps==10000:\n",
        "        if log:\n",
        "          acc,lossTot=calcAccLoss(parameters,X_train,Y_train,sizes,loss_type,activation,type=\"trng\",regu=reg)     \n",
        "          accVal,lossTotVal=calcAccLoss(parameters,X_val,Y_val,sizes,loss_type,activation)\n",
        "\n",
        "          wandb.log({\"Accuracy\":acc,\"Loss\":lossTot,\"Accuracy_val\":accVal,\"Loss_val\":lossTotVal,\"Epoch\":n,\"n_datatrain\":j + minibatch_size+n*54000})\n",
        "        steps=0     \n",
        "\n",
        "  return parameters   \n",
        "\n",
        "\n",
        "#Nadam GD\n",
        "def nadam_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=False) :\n",
        "  steps=0\n",
        "  parameters = network_init(sizes,w_init)\n",
        "  update = update_init(sizes)\n",
        "  m = update_init(sizes)\n",
        "  v = update_init(sizes)\n",
        "\n",
        "  beta1 = 0.9\n",
        "  beta2 = 0.999\n",
        "  eps = 1e-8\n",
        "\n",
        "  for n in range(n_epoch):\n",
        "\n",
        "    for j in range(0, X_train.shape[0], minibatch_size):\n",
        "      X_mini = X_train[j:j + minibatch_size]\n",
        "      Y_mini = Y_train[j:j + minibatch_size]\n",
        "      grads = grad_init(sizes)\n",
        "\n",
        "      for x,y in zip(X_mini,Y_mini):\n",
        "        y_hat,A,H = feed_forward(x,parameters,sizes,activation)\n",
        "        grads = back_prop(x,y,y_hat,grads,A,H,parameters,sizes,loss_type,activation,reg)\n",
        "\n",
        "      for i in range(1,len(sizes)-1) :                                                    #updating the parameters\n",
        "        m[\"W\"+str(i)] = beta1*m[\"W\"+str(i)] + (1-beta1)*grads[\"dW\"+str(i)]                #m_w update\n",
        "        m[\"b\"+str(i)] = beta1*m[\"b\"+str(i)] + (1-beta1)*grads[\"db\"+str(i)]                #m_b update\n",
        "\n",
        "        v[\"W\"+str(i)] = beta2*v[\"W\"+str(i)] + (1-beta2)*grads[\"dW\"+str(i)]**2             #v_w update    \n",
        "        v[\"b\"+str(i)] = beta2*v[\"b\"+str(i)] + (1-beta2)*grads[\"db\"+str(i)]**2             #v_b update\n",
        "\n",
        "        #cumulative average\n",
        "        m_w_hat = m[\"W\"+str(i)]/(1-np.power(beta1,n+1))                                   \n",
        "        m_b_hat = m[\"b\"+str(i)]/(1-np.power(beta1,n+1))\n",
        "        v_w_hat = v[\"W\"+str(i)]/(1-np.power(beta2,n+1))\n",
        "        v_b_hat = v[\"b\"+str(i)]/(1-np.power(beta2,n+1))\n",
        "\n",
        "\n",
        "        update[\"W\"+str(i)]=lr*np.multiply(np.reciprocal(np.sqrt(v_w_hat+eps)),(beta1*m_w_hat+(1-beta1)*grads[\"dW\"+str(i)]))*(1/(1-np.power(beta1,n+1)))\n",
        "        update[\"b\"+str(i)]=lr*np.multiply(np.reciprocal(np.sqrt(v_b_hat+eps)),(beta1*m_b_hat+(1-beta1)*grads[\"db\"+str(i)]))*(1/(1-np.power(beta1,n+1)))\n",
        "\n",
        "        parameters[\"W\"+str(i)] = (1-lr*reg)*parameters[\"W\"+str(i)] - update[\"W\"+str(i)]\n",
        "        parameters[\"b\"+str(i)] = (1-lr*reg)*parameters[\"b\"+str(i)] - update[\"b\"+str(i)]\n",
        "      steps=steps+1\n",
        "      if steps==10000:\n",
        "        if log:\n",
        "          acc,lossTot=calcAccLoss(parameters,X_train,Y_train,sizes,loss_type,activation,type=\"trng\",regu=reg)     \n",
        "          accVal,lossTotVal=calcAccLoss(parameters,X_val,Y_val,sizes,loss_type,activation)\n",
        "\n",
        "          wandb.log({\"Accuracy\":acc,\"Loss\":lossTot,\"Accuracy_val\":accVal,\"Loss_val\":lossTotVal,\"Epoch\":n,\"n_datatrain\":j + minibatch_size+n*54000})\n",
        "        steps=0   \n",
        "\n",
        "  return parameters   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l18nteTQXfbX"
      },
      "outputs": [],
      "source": [
        "#function to select optimizer\n",
        "def do_GD(X_train,Y_train,optimizer,activation,hl_size,input_size,output_size,n_epoch,lr,reg,w_init,loss_type,minibatch_size=1,logging=False):\n",
        "  sizes = hl_size.copy() \n",
        "  sizes.insert(0,input_size)\n",
        "  sizes.append(output_size)\n",
        "\n",
        "  if optimizer==\"sgd\":\n",
        "    return(stochastic_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=logging))\n",
        "  elif optimizer==\"momentum\":\n",
        "    return(momentum_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=logging))\n",
        "  elif optimizer==\"nesterov\":\n",
        "    return(nesterov_accelerated_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=logging))\n",
        "  elif optimizer==\"rmsprop\":\n",
        "    return(rmsprop_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=logging))\n",
        "  elif optimizer==\"adam\":\n",
        "    return(adam_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=logging))\n",
        "  elif optimizer==\"nadam\":\n",
        "    return(nadam_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=logging))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufsNzRiSXgP5"
      },
      "outputs": [],
      "source": [
        "#training function to sweep with wandb\n",
        "def train():\n",
        "\n",
        "  hyperparameter_defaults=dict(\n",
        "      input_size = 784,                                       \n",
        "      output_size = 10,                                                    \n",
        "      n_epoch = 5,                                            \n",
        "      n_hiddenlayer = 3,                               \n",
        "      hl= [64,64,64],\n",
        "      reg = 0.0005,      \n",
        "      lr = 1e-3,                                              \n",
        "      optimizer = \"momentum\",                      \n",
        "      batch_size = 64,       \n",
        "      initialization = \"xavier\",      \n",
        "      loss_type = \"cross_entropy\" \n",
        "      \n",
        "  )\n",
        "\n",
        "  wandb.init(config=hyperparameter_defaults)\n",
        "\n",
        "  config=wandb.config\n",
        "  output_size=10\n",
        "  input_size = 784                                      \n",
        "  config.hl=[config.hl_size for i in range(config.n_hiddenlayer)]   #hidden layer sizes array creation\n",
        "  parameters=do_GD(X_train,Y_train,config.optimizer,config.activation,config.hl,config.input_size,config.output_size,config.n_epoch,config.lr,config.reg,config.initialization,config.loss_type,config.batch_size,logging=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNmMXGbJYRje"
      },
      "outputs": [],
      "source": [
        "def sweeper(sweep_config,proj_name):\n",
        "  sweep_id=wandb.sweep(sweep_config,project=proj_name)\n",
        "  wandb.agent(sweep_id,train,project=proj_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRieq7Z4Zu_n"
      },
      "outputs": [],
      "source": [
        "#sweep dictionary\n",
        "sweep_config={\n",
        "    'method':'bayes',\n",
        "    'metric':{\n",
        "        'name':'accuracy',\n",
        "        'goal':'maximize'},\n",
        "\n",
        "}\n",
        "\n",
        "parameters_dict={\n",
        "    'optimizer':{\n",
        "        'values':['nadam','sgd', 'momentum', 'nesterov', 'rmsprop', 'adam']\n",
        "    },\n",
        "    'lr':{\n",
        "        'values':[1e-3,1e-5]\n",
        "    },\n",
        "    'reg':{\n",
        "        'values':[5e-4,0,5e-1]\n",
        "    },\n",
        "    'n_hiddenlayer':{\n",
        "        'values':[3,4,5]\n",
        "    },\n",
        "    'hl_size':{\n",
        "      'values':[128,32,64]  \n",
        "    },\n",
        "    'batch_size':{\n",
        "        'values':[64,32,128]\n",
        "    },\n",
        "    'loss_type':{\n",
        "        'values':['cross_entropy','squared_error']\n",
        "    },\n",
        "    'initialization':{\n",
        "        'values':['xavier','random']\n",
        "    },\n",
        "    'activation':{\n",
        "        'values':['relu','sigmoid','tanh']\n",
        "    },\n",
        "    'n_epoch':{\n",
        "        'values':[5]\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_config['parameters']=parameters_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBPSDXxaaG_2",
        "outputId": "51ba1ec3-897e-47f3-b472-bafd71921e69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: ag4ietor\n",
            "Sweep URL: https://wandb.ai/oe21d019/CS6910_ass1/sweeps/ag4ietor\n"
          ]
        }
      ],
      "source": [
        "sweep_id=wandb.sweep(sweep_config,project=proj_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "HcrqKC3WaP6o",
        "outputId": "7d0c46a8-118c-4f38-c968-286fb4909c98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 7hgrv867\n",
            "Sweep URL: https://wandb.ai/oe21d019/CS6910_ass1/sweeps/7hgrv867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: z8i9ig1x with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thl_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitialization: random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_type: squared_error\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_hiddenlayer: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: momentum\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \treg: 0.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/oe21d019/CS6910_ass1/runs/z8i9ig1x\" target=\"_blank\">fluent-sweep-1</a></strong> to <a href=\"https://wandb.ai/oe21d019/CS6910_ass1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "Sweep page: <a href=\"https://wandb.ai/oe21d019/CS6910_ass1/sweeps/7hgrv867\" target=\"_blank\">https://wandb.ai/oe21d019/CS6910_ass1/sweeps/7hgrv867</a><br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "sweeper(sweep_config,proj_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMw3yvhcaSs7"
      },
      "outputs": [],
      "source": [
        "#confusion matrix and accuracy for test values\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "hyperparameter_final=dict(\n",
        "    input_size = 784,                                       \n",
        "    output_size = 10,                                                    \n",
        "    n_epoch = 5,                                            \n",
        "    n_hiddenlayer = 3,                               \n",
        "    hl= [128,128,128],\n",
        "    reg = 0,      \n",
        "    lr = 1e-5,                                              \n",
        "    optimizer = \"nadam\",                      \n",
        "    batch_size = 128,       \n",
        "    initialization = \"xavier\",      \n",
        "    loss_type = \"cross_entropy\",\n",
        "    activation=\"relu\" \n",
        "    \n",
        ")\n",
        "\n",
        "wandb.init(config=hyperparameter_final,project=proj_name)\n",
        "config=wandb.config\n",
        "parameters_test=do_GD(X_train,Y_train,config.optimizer,config.activation,config.hl,config.input_size,config.output_size,config.n_epoch,config.lr,config.reg,config.initialization,config.loss_type,config.batch_size,logging=False)\n",
        "\n",
        "sizes = config.hl.copy() \n",
        "sizes.insert(0,config.input_size)\n",
        "sizes.append(config.output_size)\n",
        "\n",
        "Y_prob=np.empty(np.shape(y_test))\n",
        "#finding y predicted\n",
        "for i,x in enumerate(X_test):\n",
        "  Y_prob[i]= (feed_forward(x,parameters_test,sizes,config.activation)[0]).argmax()\n",
        "\n",
        "accuracy=calcAccLoss(parameters_test,X_test,Y_test,sizes,None,config.activation)[0]\n",
        "\n",
        "wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(preds=Y_prob, y_true=y_test,class_names=class_list),\"Test Accuracy\": accuracy })"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Assignmentee20d064oe21d019.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}